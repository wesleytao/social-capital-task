---
title: "Elastic Net"
author: "Wesley_Tao"
date: "2018/3/30"
output: html_document
---

# 1.Goal and Objective
The objective of this project is to dig into the Social Capital index of Counties in United States and see how it change across time. We are also interested in what is the driving factor for a high social capital index.In order to do so, we are going to build some models: <br>
1.Lasso for feature selection  <br>
2.Tree-based Classification for feature importance <br>
3.Anova Analysis test if there is significant change across time <br>
4.Boostrap for estimate distribution <br>
Hopefully, we could get positive results 

# 2.Data preprocessing and exploratory result
Some parts of putting data in a desirable format are not displayed for tidyness. But we could open linear_model.Rmd to see the code details.<br>


```{r echo=FALSE , warning=FALSE}
library(dplyr)
library(ggplot2)
Census_data <-read.csv("../data/Census data with variables 2016.csv",header = T,sep=",",as.is = T)
rate_2014   <-read.table("../output/rate_2014.tsv",header=T) # first row seems to be the description of the variable
var_des     <-Census_data[1,] # store description
Census_data <-Census_data[-1,-1] # rest of the data,drop the first column and first row





rate_2014$id          <- as.integer(rate_2014$id) # we need to convert to integer for combining data
Census_data$GEO.id2   <- as.integer(Census_data$GEO.id2)
names(rate_2014)      <- c("GEO.id2","SK14") # rename the label

states<-regmatches(Census_data$GEO.display.label,regexpr(pattern = ", [A-Z]+[a-z]+",text=Census_data$GEO.display.label))
states<-as.factor(substr(states,3,nchar(states)))
Census_data$GEO.display.label<-states
county<-gsub(",.*$", "", Census_data$GEO.display.label)
Census_data$county<-county
newdata<-rate_2014 %>%
                 left_join(Census_data,by="GEO.id2") #combine two table 
# head(newdata)
getwd()
```



But I do have to highlight some of the important parts of data processing.

### a How I choose first batch of varibles for lasso
Most of the variables in Cencus Data csv are irrelevant for our target.
```{r}
head(colnames(Census_data),10)
```
However they are encoded with a pattern. VC## represent the variable,
HC01 is the Estimate which is a real positive number <br>
HC02 is the Margin of the error <br>
HC03 is HC01/Population <br>
HC04 is HC02/Population <br>
Then the problem boils down to find the pattern of "HC03_VC##"

```{r echo=TRUE, warning=FALSE}
X_pattern       <-"HC03_VC[0-9]+"# regular expression
newdata_colnames<-names(newdata)
X_index         <-grep(X_pattern,newdata_colnames)
head(newdata_colnames[X_index],10) # but HC03_VC_03 is the population
X_index
```


There are also part of the data is in the form of (X), so we get rid of them too.
```{r}
head(newdata[,"HC03_VC118"])
```
```{r}
newdata<-na.omit(newdata)
var_des[4]
pop<-as.numeric(newdata$HC01_VC03) # this variable is the population estimate

```
```{r}
length(pop)
```

```{r echo=FALSE, warning=FALSE}
X_index     <-X_index[-1] # remove the first HC03_VC_03
y_index     <-2
subset_index<-c(y_index,X_index) # combine with y
newdata<-apply(newdata[,subset_index],2,as.numeric)# these '(X)' will be automatically handled by as.integer and replaced by NA

nrow(newdata)
head(newdata)
```

```{r}
nrow(newdata)==length(pop)
# divide by population 
# we need to find insurance coverage 

columncasewisedivide<-function(nominator,denominator){
  return(nominator/denominator)
}
X<-apply(newdata[,4:ncol(newdata)],2,columncasewisedivide,denominator=pop)
# newdata[1:10,4:ncol(newdata)]
cleaned_data<-cbind(newdata[,1],X)

```
```{r}
# center and scale first
scaled_data<-scale(cleaned_data, center = TRUE, scale = TRUE)
?scale
# head(scaled_data)
ncol(scaled_data)

# get rid of NA values
scaled_data<-scaled_data[,colSums(!is.na(scaled_data)) > 0]
require(stats)
x <- matrix(1:10, ncol = 2)
x
(centered.x <- scale(x, scale = FALSE))
cov(centered.scaled.x <- scale(x)) # all 1
```


### b how I deal with the 100 varibles 
There are 100 varibles. These are the varibles HC01 which is already divided by the population. 
for example HC03_VC04 is the employment rate.

## 2.2 Data Description
# 3.Exploratory result
```{r echo=TRUE}
# class(pop)
setwd("../figs") # set image path
png(filename = paste("population",".png",sep = ""))
ggplot(data=data.frame(pop=log(pop)))+
  geom_histogram(aes(x=pop))+
  labs(x="log_population",title="population distribution of US")
dev.off()

```

```{r echo=TRUE, warning=FALSE}
library(reshape)
merged<-read.csv("../output/merged_4_year.csv")
merged<-merged[,-1]
merged.melt<-melt(merged,id.vars = "fips",variable_name = "year",value_name="index")
merged.melt$year<-as.factor(merged.melt$year)


setwd("../figs") # set image path
png(filename = paste("Socail Capital",".png",sep = ""))

ggplot(data=merged.melt,aes(x=value,fill=year))+
  geom_density(alpha=0.3)+
  labs(x="index",title="Social Capital index for each year")
dev.off()
```

```{r}
head(merged.melt)
merged.melt<-na.omit(merged.melt)
tapply(merged.melt$value,merged.melt$year,mean)

```

# Train and test split
```{r}
# split the data for train and test 
# 20% of the data as test data and 80 % as train data 
set.seed(1)
scaled_data<-na.omit(scaled_data)

total.num      <-nrow(scaled_data)
test_index_row <-sample(seq(total.num),total.num*0.2)
train_index_row<-setdiff(seq(total.num),test_index_row)

X_test<-scaled_data[test_index_row,-1]
y_test<-scaled_data[test_index_row,1]

X_train<-scaled_data[train_index_row,-1]
y_train<-scaled_data[train_index_row,1]

length(test_index_row)+length(train_index_row)==total.num # make sure the number meet


# sum(is.na(X_train))
```

# Heat map of variable correlation
```{r}
library(ggplot2)
library(reshape2)
cormat       <-round(cor(X_train),2)
melted_cormat<-melt(cormat) # transform to a narrow format
# head(melted_cormat,3)
ggplot(data = melted_cormat, aes(x=X1, y=X2, fill=value)) + 
  geom_tile()
# it seems that this data set has a lot of collinearity, It might cause a lot of problem, we need regularization 

```

# Elastic Net for variable selection
My original idea is to use lasso for variable selection, however , we can see clearly that some of the X variables are highly correlated thus regularizaiton of L2 penalty is needed.
for more details about the loss function of the model:

$$
\min_{\beta_0,\beta}(\frac{1}{2N}\sum_{i=1}^{N}(y_i-\beta_0-x_i^T\beta)^2+\lambda[(1-\alpha)||\beta||_2^2/2+\alpha||\beta||_1)])
$$
There are two parameters, one is $\alpha$ another is $\lambda$ . $\alpha$ is a compromise between Ridge($\alpha=0$) and Lasso regression ($\alpha=1$). 
We choose the ideal alpha based on 10-fold cross-validation.
$\lambda$ is the penalty we put on the parameter.


```{r message=FALSE}
library(glmnet)
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
set.seed(4)
alpha_seq<-seq(10)/10

result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))# generate a data frame to store the result

for(i in 1:length(alpha_seq)){
  this.alpha <-alpha_seq[i]
  fit.reg.cv <- cv.glmnet(X_train, y_train,nfold=10, type.measure="mse", alpha=this.alpha, 
                          family="gaussian")
  this.lambda<-fit.reg.cv$lambda.min # we choose lambda with the minimun mse 
  
  yhat       <-predict(fit.reg.cv, s=this.lambda, newx=X_train) # get pred over the train set
  mse        <- mean((y_train - yhat)^2) #compute mse
  result[i,2]<-mse #
  
}

# plot mse over alpha

setwd("../figs") # set image path
png(filename = paste("cross-validation error_scaled",".png",sep = ""))

ggplot(data = result,aes(x=alpha,y=mse))+
  geom_line()+
  labs(title="Select Alpha based on cross-validation error")

```

it seems that lasso perform better than ridge regression. based on the pics above We are going to select alpha=0.9


```{r}
set.seed(1)
fit.ela.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0.9,
                          family="gaussian",nfolds = 5)

setwd("../figs") # set image path
png(filename = paste("select lambda",".png",sep = ""))
plot(fit.ela.cv)
dev.off()


yhat <- predict(fit.ela.cv, s=fit.ela.cv$lambda.1se, newx=X_test)
test_mse <- mean((y_test - yhat)^2)

```

```{r}
test_mse
```


But based on the graph above, we can select top 10 variables which might have statistical significant influence over the social capital index


```{r}
# we are interested in those variabels
head(data.frame(n=fit.ela.cv$nzero,lambda=fit.ela.cv$lambda),20)
```

Based on the table above, I am going to select top 10 variables lambda=0.10475557
```{r}
# use full data
final_model<-glmnet(scaled_data[,-1], scaled_data[,1], alpha=0.9, lambda=0.10475557,
                          family="gaussian")
head(final_model$beta,10) # as we can see many variables are shrink to seros
scaled_data[,1]
```

we are interested in those non-zero varariables 
```{r}
myvar<-which(final_model$beta!=0)
# final_model$beta
x_names<-colnames(cleaned_data)[-1]
data.frame(code=x_names[myvar],coe=final_model$beta[myvar])

```

we are also interested what are these 
```{r}
dex_index<-which(colnames(var_des) %in% x_names[myvar])
t(var_des[,dex_index])
```
# Conclusion
We found several varibles which are highly correlated to the social capital index, they are listed below. 
 
1. Employment status          (+)  
2. Unemployed rate            (-) 
3. Health Insurance Coverage  (+)
4. No Health cover            (-)
5. Below Poverty              (-)
```{r}
library(gbm)
instal
```








X_index
X_index[-1]
X_index<-X_index[-1] # remove the first HC03_VC_03
# split the data for train and test
#
head(newdata)
# split the data for train and test
# 20% of the data as test data and 80 % as train data
head(newdata)
nrow(newdata)
total.num<-nrow(newdata)
seq(total.num)
sample(seq(total.num),total.num*0.2)
test_index <-sample(seq(total.num),total.num*0.2)
train_index<-setdiff(seq(total.num),test_index)
length(test_index)
length(test_index)+length(train_index)
length(test_index)+length(train_index)==total.num
X_train<-newdata[train_index,X_index]
y_train<-newdata[train_index,"SK14"]
library(glmnet)
library(glmnet)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian")
str(y_train)
str(X_train)
# we also find some variables have (X) we have to get rid of them
newdata[,HC03_VC118]
# we also find some variables have (X) we have to get rid of them
head(newdata[,"HC03_VC118"])
as.integer(newdata[,"HC03_VC118"])
as.integer(newdata[,X_index])
head(newdata[,X_index])
apply(head(newdata[,X_index]),2,as.integer)
X_index
head(newdata)
subset_index<-c(y_index,X_index)
y_index<-2
subset_index<-c(y_index,X_index)
newdata.subset<-apply(newdata[,subset_index],2,as.integer)# these '(X)' will be automatically handled by as.integer and replaced by NA
head(newdata.subset)
newdata.subset<-apply(newdata[,subset_index],2,as.numeric)# these '(X)' will be automatically handled by as.integer and replaced by NA
head(newdata.subset)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean)
?mean
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean(na.rm=T))
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean(na.rm=T))
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean,na.rm=T)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean,na.rm=T)>200
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean,na.rm=T)<200
newdata.subset[,my_x]
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
my_x<-apply(newdata.subset,2,mean,na.rm=T)<200
heas(newdata.subset[,my_x])
head(newdata.subset[,my_x])
my_x
apply(my_x,1,replace_na)
replace_na<-function(x){
if (is.na(x)==T){return(FALSE)}
else  return (x)
}
apply(my_x,1,replace_na)
apply(my_x,2,replace_na)
tapply(my_x,1,replace_na)
tapply(my_x,2,replace_na)
na.omit(my.x)
na.omit(my_x)
head(newdata.subset[,na.omit(my_x)])
newdata<-newdata.subset[,na.omit(my_x)]
# split the data for train and test
# 20% of the data as test data and 80 % as train data
total.num<-nrow(newdata)
test_index <-sample(seq(total.num),total.num*0.2)
train_index<-setdiff(seq(total.num),test_index)
length(test_index)+length(train_index)==total.num
X_train<-newdata[train_index,X_index]
head(newdata)
my_x[!is.na]
my_x[!is.na(my_x)]
newdata<-newdata.subset[,my_x[!is.na(my_x)]]
head(newdata)
my_x
my_x[!is.na(my_x)]
which(my_x[!is.na(my_x)])
newdata.1<-newdata.subset[,which(my_x[!is.na(my_x)])]
head(newdata.1)
# we also find some variables have (X) we have to get rid of them
head(newdata[,"HC03_VC118"])
Census_data<-read.csv("../data/Census data with variables 2016.csv",header = T,sep=",",as.is = T)
rate_2014  <-read.table("../output/rate_2014.tsv",header=T)
# first row seems to be the description of the variable
var_des    <-Census_data[1,] # store description
Census_data<-Census_data[-1,-1] # rest of the data,drop the first column
head(Census_data)
library(dplyr)
# combine the data of social capital index with cencus
# head(Census_data)
# head(rate_2014)
rate_2014$id          <- as.integer(rate_2014$id)
Census_data$GEO.id2   <- as.integer(Census_data$GEO.id2)
names(rate_2014)      <-c("GEO.id2","SK14")
setdiff(rate_2014$GEO.id2,Census_data$GEO.id2) # geocode  2270 46113 are mismatched
newdata<-rate_2014 %>%
left_join(Census_data,by="GEO.id2") # we  are interested in sk2014
# we dig into the cencus data we found that HC03_VC03 is the population
#                                    and HC03_VC(##) is the ## variable divided by population
# therefore we select those variables HC03_VC(##)
X_pattern       <-"HC03_VC[0-9]+"
newdata_colnames<-names(newdata)
X_index         <-grep(X_pattern,newdata_colnames)
head(newdata_colnames[X_index]) # but HC03_VC_03 is the population
X_index<-X_index[-1] # remove the first HC03_VC_03
y_index<-2
subset_index<-c(y_index,X_index)
C
# we also find some variables have (X) we have to get rid of them
head(newdata[,"HC03_VC118"])
newdata.subset<-apply(newdata[,subset_index],2,as.numeric)# these '(X)' will be automatically handled by as.integer and replaced by NA
head(newdata.subset)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean,na.rm=T)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
apply(newdata.subset,2,mean,na.rm=T)<200
which(log.index)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
log.index<-apply(newdata.subset,2,mean,na.rm=T)<200
which(log.index)
new_index<-which(log.index)
newdata.subset[,new_index]
cleaned_data<-newdata.subset[,new_index]
total.num      <-nrow(cleaned_data)
test_index_row <-sample(seq(total.num),total.num*0.2)
train_index_row<-setdiff(seq(total.num),test_index_row)
length(test_index_row)+length(train_index_row)==total.num
X_train<-newdata[train_index_row,-1]
y_train<-newdata[train_index_row,1]
library(glmnet)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian")
x_train
X_train
head(X_train)
X_train<-cleaned_data[train_index_row,-1]
y_train<-cleaned_data[train_index_row,1]
library(glmnet)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian")
cor(X_train)
?cv.glmnet
options("na.action")
na.action(c(1,2,3))
na.action(c(1,NA,3))
options("na.action")
mean(X_train)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian",na.action=T)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian",na.omit=T)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian",na.omit=T)
na.omit(c(1,NA,3))
Census_data<-read.csv("../data/Census data with variables 2016.csv",header = T,sep=",",as.is = T)
rate_2014  <-read.table("../output/rate_2014.tsv",header=T)
# first row seems to be the description of the variable
var_des    <-Census_data[1,] # store description
Census_data<-Census_data[-1,-1] # rest of the data,drop the first column
head(Census_data)
library(dplyr)
# combine the data of social capital index with cencus
# head(Census_data)
# head(rate_2014)
rate_2014$id          <- as.integer(rate_2014$id)
Census_data$GEO.id2   <- as.integer(Census_data$GEO.id2)
names(rate_2014)      <-c("GEO.id2","SK14")
setdiff(rate_2014$GEO.id2,Census_data$GEO.id2) # geocode  2270 46113 are mismatched
newdata<-rate_2014 %>%
left_join(Census_data,by="GEO.id2") # we  are interested in sk2014
# we dig into the cencus data we found that HC03_VC03 is the population
#                                    and HC03_VC(##) is the ## variable divided by population
# therefore we select those variables HC03_VC(##)
X_pattern       <-"HC03_VC[0-9]+"
newdata_colnames<-names(newdata)
X_index         <-grep(X_pattern,newdata_colnames)
head(newdata_colnames[X_index]) # but HC03_VC_03 is the population
X_index<-X_index[-1] # remove the first HC03_VC_03
y_index<-2
subset_index<-c(y_index,X_index)
# we also find some variables have (X) we have to get rid of them
head(newdata[,"HC03_VC118"])
newdata.subset<-apply(newdata[,subset_index],2,as.numeric)# these '(X)' will be automatically handled by as.integer and replaced by NA
head(newdata.subset)
# it seems that we also have some variables which are population labor force, we have to get rid of them too
# rules are if value is larger than 200 then
log.index<-apply(newdata.subset,2,mean,na.rm=T)<200
new_index<-which(log.index)
cleaned_data<-newdata.subset[,new_index]
cleaned_data<-na.omit(cleaned_data) # omit na by row
# split the data for train and test
# 20% of the data as test data and 80 % as train data
total.num      <-nrow(cleaned_data)
test_index_row <-sample(seq(total.num),total.num*0.2)
train_index_row<-setdiff(seq(total.num),test_index_row)
length(test_index_row)+length(train_index_row)==total.num
X_train<-cleaned_data[train_index_row,-1]
y_train<-cleaned_data[train_index_row,1]
library(glmnet)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=1,
family="gaussian")
summary(fit.lasso.cv)
plot(fit.lasso.cv)
fit.lasso.cv
1
1
X_test<-cleaned_data[test_index_row,-1]
y_test<-cleaned_data[test_index_row,1]
# test data
yhat <- predict(fit.lasso.cv, s=fit.lasso.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
fit.lasso.cv$lambda.1se
fit.lasso.cv$glmnet.fit
fit.lasso.cv$lambda.min
fit.lasso.cv$name
fit.lasso.cv$cvm
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=1,
family="gaussian")
plot(fit.lasso.cv)
fit.lasso.cv$cvm
# test data
yhat <- predict(fit.lasso.cv, s=fit.lasso.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
# it seems lasso tend to overfit the problem
fit.ridge.cv <- cv.glmnet(x_train, y_train, type.measure="mse", alpha=0,
family="gaussian")
# it seems lasso tend to overfit the problem
fit.ridge.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0,
family="gaussian")
# it seems lasso tend to overfit the problem
fit.ridge.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0,
family="gaussian",nfolds = 5)
plot(fit.ridge.cv)
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
# elastic net
fit.ela.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0.5,
family="gaussian",nfolds = 5)
plot(fit.ela.cv)
yhat <- predict(fit.ridge.cv, s=fit.ridge.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
fit.lasso.cv$lambda.min
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.min, newx=X_test)
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.min, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.lse, newx=X_test)
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.lse, newx=X_test)
yhat <- predict(fit.ridge.cv, s=fit.lasso.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
library(glmnet)
# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=1,
family="gaussian")
plot(fit.lasso.cv)
fit.lasso.cv$lambda
fit.lasso.cv$nzero
cor(X_train)
plot(cor(X_train))
ggcoplot(cor(X_train))
ggcorlot(cor(X_train))
ggcorrplot(cor(X_train))
library(ggplot2)
ggcorrplot(cor(X_train))
library(ggplot2)
library(reshape2)
cormat<-round(cor(X_train),2)
melted_cormat<-melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile()
fit.lasso.cv$name
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
seq(10)/10
result<-data.frame(rep(NA,2*length(alpha_seq)),ncol=2)
0
result<-data.frame(rep(NA,2*length(alpha_seq)),ncol=2)
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
alpha_seq<-seq(10)/10
result<-data.frame(rep(NA,2*length(alpha_seq)),ncol=2)
result
result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))
result
alpha_seq<-seq(10)/10
result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))
# result
for(i in 1:length(alpha_seq)){
this.alpha <-alpha_seq[i]
fit.reg.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=this.alpha,
family="gaussian")
result[i,2]<-fit.lasso.cv$cvm
}
fit.lasso.cv$cvm
fit.lasso.cv$lse
fit.lasso.cv$1se
fit.lasso.cv$ise
fit.lasso.cv$Ise
fit.lasso.cv$lse
fit.lasso.cv$1se
fit.lasso.cv$1se
fit.lasso.cv$1se
fit.lasso.cv$lse
fit.lasso.cv$1se
fit.lasso.cv$1se
fit.lasso.cv$lamda.1se
fit.lasso.cv$lambda.1se
library(glmnet)
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
alpha_seq<-seq(10)/10
result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))
# result
for(i in 1:length(alpha_seq)){
this.alpha <-alpha_seq[i]
fit.reg.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=this.alpha,
family="gaussian")
this.lambda<-fit.reg.cv$lambda.min # we choose lambda with the minimun mse
yhat       <-predict(fit.reg.cv, s=fit.reg.cv$this.lambda, newx=X_test) # get pred over the train set
mse <- mean((y_train - yhat)^2) #compute mse
result[i,2]<-mse #
}
library(glmnet)
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
alpha_seq<-seq(10)/10
result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))
# result
for(i in 1:length(alpha_seq)){
this.alpha <-alpha_seq[i]
fit.reg.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=this.alpha,
family="gaussian")
this.lambda<-fit.reg.cv$lambda.min # we choose lambda with the minimun mse
yhat       <-predict(fit.reg.cv, s=this.lambda, newx=X_test) # get pred over the train set
mse <- mean((y_train - yhat)^2) #compute mse
result[i,2]<-mse #
}
library(glmnet)
# 5-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
alpha_seq<-seq(10)/10
result<-data.frame(alpha=alpha_seq,mse=rep(NA,length(alpha_seq)))
# result
for(i in 1:length(alpha_seq)){
this.alpha <-alpha_seq[i]
fit.reg.cv <- cv.glmnet(X_train, y_train,nfold=5, type.measure="mse", alpha=this.alpha,
family="gaussian")
this.lambda<-fit.reg.cv$lambda.min # we choose lambda with the minimun mse
yhat       <-predict(fit.reg.cv, s=this.lambda, newx=X_train) # get pred over the train set
mse <- mean((y_train - yhat)^2) #compute mse
result[i,2]<-mse #
}
plot(result)
ggplot(data = result,aes(x=alpha,y=mse))+
geom_line()
ggplot(data = result,aes(x=alpha,y=mse))+
geom_line()+
labs(title="cross-validation error: select alpha")
ggplot(data = result,aes(x=alpha,y=mse))+
geom_line()+
labs(title="Select alphaï¼šcross-validation error")
# it seems lasso tend to overfit the problem
fit.ridge.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0.5,
family="gaussian",nfolds = 5)
plot(fit.ridge.cv)
# elastic net
fit.ela.cv <- cv.glmnet(X_train, y_train, type.measure="mse", alpha=0.5,
family="gaussian",nfolds = 5)
plot(fit.ela.cv)
# test mse
yhat <- predict(fit.ela.cv, s=fit.ela.cv$lambda.lse, newx=X_test)
# test mse
yhat <- predict(fit.ela.cv, s=fit.ela.cv$lambda.lse, newx=X_test)
plot(fit.ela.cv)
# test mse
fit.ela.cv$lambda.lse
# test mse
fit.ela.cv$lambda.min
# test mse
fit.ela.cv$lambda.1se
# test mse
yhat <- predict(fit.ela.cv, s=fit.ela.cv$lambda.1se, newx=X_test)
# test mse
yhat <- predict(fit.ela.cv, s=fit.ela.cv$lambda.1se, newx=X_test)
mse <- mean((y_test - yhat)^2)
mse
# we are interested in those variabels
fit.ela.cv$lambda
fit.ela.cv$nzero
# we are interested in those variabels
fit.ela.cv$lambda
# we are interested in those variabels
plot(log(fit.ela.cv$lambda),fit.ela.cv$nzero)
# we are interested in those variabels
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
hline(x=10)
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
vline(x=10)
# we are interested in those variabels
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
abline(x=10)
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
abline(h=10)
# we are interested in those variabels
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
abline(v=10)+
xlab("number of variable selected")+
ylab("lambda")
# we are interested in those variabels
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
abline(v=10)
# xlab("number of variable selected")+
# ylab("lambda")
# we are interested in those variabels
plot(fit.ela.cv$nzero,log(fit.ela.cv$lambda.lse))+
abline(v=10)
# we are interested in those variabels
plot(fit.ela.cv$nzero,fit.ela.cv$lambda.lse)+
abline(v=10)
# xlab("number of variable selected")+
# ylab("lambda")
# we are interested in those variabels
fit.ela.cv$nzero
# we are interested in those variabels
data.frame(n=fit.ela.cv$nzero,lambda=fit.ela.cv$lambda)
glmnet(X_train, y_train, type.measure="mse", alpha=0.5, lambda=0.5360136197,
family="gaussian")
glmnet(X_train, y_train, alpha=0.5, lambda=0.5360136197,
family="gaussian")
final_model<-glmnet(X_train, y_train, alpha=0.5, lambda=0.5360136197,
family="gaussian")
summary(final_model)
final_model$beta
final_model$
final_model$a0
final_model$nobs
final_model$beta
head(final_model$beta)
head(final_model$beta,10)
final_model$beta
class(final_model$beta)
final_model$beta[1]
final_model$beta[2]
final_model$beta[4]
myvar<-final_model$beta!=0
final_model$beta[myvar]
which(myvar)
myvar<-which(final_model$beta!=0)
final_model$beta[myvar]
names(final_model$beta)
final_model$beta
# use full data
final_model<-glmnet(cleaned_data[,-1], cleaned_data[,1], alpha=0.5, lambda=0.5360136197,
family="gaussian")
head(final_model$beta,10) # as we can see many variabel are shrink to seros
myvar<-which(final_model$beta!=0)
final_model$beta
data.frame(myvar,coe=final_model$beta[myvar])
names(cleaned_data)[myvar]
names(cleaned_data)
colnames(cleaned_data)
colnames(cleaned_data)[-1]
data.frame(x_names[myvar],coe=final_model$beta[myvar])
x_names<-colnames(cleaned_data)[-1]
data.frame(code=x_names[myvar],coe=final_model$beta[myvar])
var_des
x_names[myvar] %in% colnames(var_des)
colnames(var_des) %in% x_names[myvar]
colnames(var_des) %in% x_names[myvar]
which(colnames(var_des) %in% x_names[myvar])
dex_index<-which(colnames(var_des) %in% x_names[myvar])
colnames(var_des)[dex_index]
var_des[,dex_index]
var_des[,dex_index].T
var_des[,dex_index]
melt(var_des[,dex_index])
as.vector(var_des[,dex_index])
t(var_des[,dex_index])
t(var_des[,dex_index])[1]
t(var_des[,dex_index])
